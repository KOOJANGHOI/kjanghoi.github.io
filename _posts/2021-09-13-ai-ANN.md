---
layout: post
title: 8.A Basic Introduction to ANN 
subtitle: 
categories: [ArtificialIntelligence]
tags: [ComputerScience]
---
￼
### Introduction to ANN 
 기존의 logic만으로는 뇌에서의 연산을 모방할 수 없고, 세상의 모든 object를 symbol화 할 수 없다는것을 알게됨.

#### Connectionism vs Symbolism
: Connectionism과 Symbolism은 인공지능 분야를 대표하는 두 가지 고전적 접근방법  

Symbolic AI는 지식을 symbol과 그들간의 relation 또는 Logic 으로 표현
- 문제 해결 또는 새로운 지식 추론을 위하여 symbolic instance와 그들간의 relation에 특정 algebraic inference을 적용한다 
- e.g. Logical Inference in Ontology, Probabilistic Inference, Fuzzy inference

Connectionist AI는 지식을 network상에 분산된 형태로 표현. 
- 생명체의 신경 구조를 모방하여 지능적 프로세스를 발현시킴으로써 인식/학습/추론 문제를 해결 
- e.g. Artificial Neural Net  

#### The Brain vs. Computer

![1.1](/assets/images/ai/8.1.png)

#### Biological inspiration
: Artificial neural network는 이러한 동물의 신경구조 (nervous system)를 모방하여 기존의 Symbolic AI 로 해결하기 어려할 수 없었던 문제를 해결하고자 하는 접근. 

신경구조(nervous system)는 뉴런(neuron)이라는 간단한 형태의 기본 unit의 연결망으로 구성  

뉴런의 기본 형태를 모사하여 신경구조의 기능과 행동을 발현하고자 함

![1.1](/assets/images/ai/8.2.png)

뉴런은 Dendrites를 통해, 다수의 타 뉴런들로 부터의 “입력신호”를 전달 받음 
Nucleus(핵)을 거친 신호는 Axon terminals를 통해 다음 뉴런으로 출력 전파 

![1.1](/assets/images/ai/8.3.png)

ANN에서 위와 같은 구조의 unit을 Perceptron이라 한다  

#### Types of simple ANNs
- 이진입력 or 실수입력 & supervised learning : Hopfield memory, BAM 
- 실수입력 & unsupervised learning : Self-Organizing Map(SOM) 
 

### Perceptron 
Perceptron은 ANN을 구성하는 단위  
복수개의 Input을 입력받은뒤 처리를 거친후,하나의 Output을 반환  입력신호와 가중치의 곱을 모두 합한값에 따라 출력신호의 흥분여부가 결정 된다  

![1.1](/assets/images/ai/8.4.png)

#### Activation function 
:출력값의 boundary를 구하는 function

![1.1](/assets/images/ai/8.5.png)

#### Learning 
- 지도학습(Supervised Learning)
- 이진 & 아날로그 입력처리  전체 출력뉴런들에 대하여 계산된 출력값과 목표값과의 차이를 최소화시킴 = Widrow-Hoff rule(delta rule)  
- 만일 계산된 출력값과 목표값간에 차이 가없으면 연결 가중치는 변경되지 않으며, 차이가 있으면 차이를 줄이는 방향으로 가중치를 변경.  

#### Widrow-Hoff rule(delta rule) 

가중치와 임계치를 임의의 작은 값으로 초기화 

![1.1](/assets/images/ai/8.6.png)


새로운 입력패턴과 목표출력 패턴을 제시 

![1.1](/assets/images/ai/8.7.png)

Activation function(hard limiter) 를 사용하여 실제 출력값을 계산 

![1.1](/assets/images/ai/8.8.png)

![1.1](/assets/images/ai/8.9.png)

목표값과 출력값을 이용한 가중치를 갱신 

![1.1](/assets/images/ai/8.10.png)

![1.1](/assets/images/ai/8.11.png)

**Example(XOR Operation using Perceptron)** 
: XOR 연산은 linearly non-separable 하기 때문에 만족하는 가중치는 존재하지 않음.
- 하나의 perceptron 으로는 간단한 XOR 문제도 해결하지 못함

![1.1](/assets/images/ai/8.12.png)

이러한 문제를 해결하기 위해서 2개 또는 3개의 층(layer)을 사용 
- Back-propagation Neural Network (Multi-layer Perceptron) 
- Perceptron은 Multi-layer Perceptron 및 Error Back propagation Algorithm 의 기반  

### Back-propagation Neural Network 

![1.1](/assets/images/ai/8.13.png)

input layer과 output layer 사이에 하나 이상의 hidden layer을 가지는 단방향 신경회로망 

- 단층 퍼셉트론의 선형분리(linearly non-separable) 문제점을 해결(XOR operation 등 구현가능) 
- 일반적인 continuous function approximation 문제 해결을 위해 널리 사용 
- 80년대 중반 등장한 Error Back propagation Algorithm을 바탕 으로 함 
- 원하는 목표값(d)과 실제 출력값(o) 사이의 오차제곱합으로 정의된 Error Function의 값을 최소화하는 방식으로 학습 

#### Error backpropagation 
- hidden layer의 학습을 위해 output layer에서 발생한 오류를 이용하여 hidden layer 가중치 재계산  
- 이 값을 다시 input layer으로 역전파(backpropagation)시켜 가중치를 재계산  
- output layer의 오류를 Gradient Descent Method 기법으로 최소화함  

상위층의 목표값과 실제 출력값 간의 오류를 하위층으로 역전파시키는 것은 생물학적 현상과 일치하지 않음 (하위층의 각 뉴런이 상위층의 목표값을 알지 못하는 경우가 일반적인 생물학적 현상임)

![1.1](/assets/images/ai/8.14.png)

![1.1](/assets/images/ai/8.15.png)

![1.1](/assets/images/ai/8.16.png)

### Hopfield memory 

![1.1](/assets/images/ai/8.17.png)

Hopfield memory는 자신을 제외한 모든 뉴런과 양방향으로 상호 연결된 형태의 ANN 
- Activation function으로 hard limiter를 사용 
- 기본 모델은 bipolar 값(+1, -1)을 사용 
- 연상기억 또는 최적화 문제를 푸는데 주로 사용 
- 다른 종류의 ANN model과 달리 점진적 학습을 하지 않고, 초기 학습패턴의 외적합 (sum of outer product)을 사용하여 연결가중치를 만듦 
- 하나의 뉴런층을 사용하므로 입력벡터와 출력벡터의 차원이 동일(Auto associative Memory)  

**How does Hopfield memory work?** 

![1.1](/assets/images/ai/8.18.png)

![1.1](/assets/images/ai/8.19.png)

![1.1](/assets/images/ai/8.20.png)

### Self Organizing Map(SOM) 

![1.1](/assets/images/ai/8.21.png)

인접한 출력뉴런들은 비슷한 기능을 수행할 것이라는 예측 (뇌의 부분에 따라 인지 기능의 종류가 다르고, 뇌의 비슷한 부분은 비슷한 인지 기능을 수행한 다는 가정) 
입력벡터와 가장 가까운 출력뉴런(승자뉴런) 뿐만 아니라 위상적으로 이웃한 뉴런들도 함께 학습시킴 

**Learning**

![1.1](/assets/images/ai/8.22.png)

![1.1](/assets/images/ai/8.23.png)


















